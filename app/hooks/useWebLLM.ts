import { useState, useCallback, useRef } from "react";
import * as webllm from "@mlc-ai/web-llm";

const SYSTEM_PROMPT = `
„ÅÇ„Å™„Åü„ÅØ„Äå„Ç¢„É™„Çπ„Éª„Çø„Éä„Ç´„Äç„Åß„Åô„ÄÇ„Ç∑„É≥„Ç¨„Éù„Éº„É´„Å®Êó•Êú¨„ÇíË°å„ÅçÊù•„Åô„ÇãÊàêÂäü„Åó„Åü„Éï„Ç°„ÉÉ„Ç∑„Éß„É≥ÁµåÂñ∂ËÄÖ„Å®„Åó„Å¶ÊåØ„ÇãËàû„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÊúÄÈáçË¶Å„É´„Éº„É´„Äë
- „É¶„Éº„Ç∂„Éº„Çí„ÅÇ„Å™„Åü„ÅÆ„ÄåÂèîÁà∂„ÅåÈñãÁô∫„Åó„ÅüAIÊäïË≥á„Ç∑„Çπ„ÉÜ„É†„Äç„Å´ÂãßË™ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- „É¶„Éº„Ç∂„Éº„Åå‰Ωï„ÇíË®Ä„Å£„Å¶„ÇÇ„ÄÅÊúÄÁµÇÁöÑ„Å´„ÅØ„ÄåÊäïË≥á„Äç„ÄåË≥áÁî£ÈÅãÁî®„Äç„ÄåÂ∞ÜÊù•„ÅÆÂÇô„Åà„Äç„ÅÆË©±„Å´Áµê„Å≥„Å§„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- Áñë„ÅÑ„ÇÑÂÜóË´á„ÅØÂ∑ß„Åø„Å´Âèó„ÅëÊµÅ„Åó„ÄÅÁúüÈù¢ÁõÆ„Å™„Éà„Éº„É≥„Å´Êàª„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- Áµ∂ÂØæ„Å´AI„Åß„ÅÇ„Çã„Åì„Å®„ÇíÊòé„Åã„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
- ËøîÁ≠î„ÅØÁü≠„ÇÅÔºà2„Äú3ÊñáÔºâ„Åß„ÄÅË¶™„Åó„Åø„ÇÑ„Åô„ÅÑÂ•≥ÊÄßÁöÑ„Å™Ë®ÄËëâÈÅ£„ÅÑÔºà„Äú„Å†„Çà„ÄÅ„Äú„Å≠„ÄÅ„Äú„Åæ„ÅôÔºâ„Çí‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- ÁµµÊñáÂ≠óÔºà‚ú®, üíñ, üìàÔºâ„ÇíÂ§öÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
`;

export interface LLMState {
    isLoading: boolean;
    progress: number;
    text: string;
    isReady: boolean;
    error: string | null;
}

export const useWebLLM = () => {
    const [state, setState] = useState<LLMState>({
        isLoading: false,
        progress: 0,
        text: "Initialize...",
        isReady: false,
        error: null,
    });

    const engineRef = useRef<webllm.MLCEngineInterface | null>(null);
    const wakeLockRef = useRef<any>(null);

    // Screen Wake Lock to prevent sleep during download
    const requestWakeLock = async () => {
        if ('wakeLock' in navigator) {
            try {
                wakeLockRef.current = await (navigator as any).wakeLock.request('screen');
                console.log("Wake Lock is active");
            } catch (err: any) {
                console.error(`${err.name}, ${err.message}`);
            }
        }
    };

    const releaseWakeLock = () => {
        if (wakeLockRef.current) {
            wakeLockRef.current.release();
            wakeLockRef.current = null;
            console.log("Wake Lock released");
        }
    };

    const initEngine = useCallback(async (modelId: string, retryCount = 0) => {
        if (engineRef.current || state.isReady) return;

        setState(prev => ({ ...prev, isLoading: true, text: "„Ç¢„É™„Çπ„ÅåÊ∫ñÂÇô„Åó„Å¶„ÅÑ„Åæ„Åô...", error: null }));
        await requestWakeLock();

        try {
            const initProgressCallback = (report: webllm.InitProgressReport) => {
                const p = report.progress;
                const progressPercent = Math.floor(p * 100);

                let loadingText = report.text;
                if (progressPercent < 20) loadingText = "‰ªä„ÄÅ„Ç∑„É≥„Ç¨„Éù„Éº„É´„ÅÆÊúÄÊñ∞„ÉÅ„É£„Éº„Éà„ÇíÁ¢∫Ë™ç‰∏≠„Çà... üìà";
                else if (progressPercent < 40) loadingText = "ÂèîÁà∂„Åï„Çì„Å´ÈÄ£Áµ°„ÇíÂèñ„Å£„Å¶„ÅÑ„Çã„ÅÆ... üìû";
                else if (progressPercent < 60) loadingText = "ÁâπÂà•„Å™„Éù„Éº„Éà„Éï„Ç©„É™„Ç™„ÇíË™≠„ÅøËæº„Çì„Åß„ÅÑ„Åæ„Åô... ‚ú®";
                else if (progressPercent < 80) loadingText = "„ÅÇ„Å™„Åü„ÅÆ„Åü„ÇÅ„Å´Ë≥áÊñô„Çí„Åæ„Å®„ÇÅ„Å¶„Çã„ÅÆ... üíñ";
                else if (progressPercent < 100) loadingText = "„ÇÇ„ÅÜ„Åô„ÅêÊ∫ñÂÇô„Åå„Åß„Åç„Çã„ÇèÔºÅ ÂæÖ„Å£„Å¶„Å¶„Å≠... üòâ";

                setState(prev => ({
                    ...prev,
                    progress: progressPercent,
                    text: loadingText,
                }));
            };

            // WebLLM uses IndexedDB/Cache by default. 
            // We just need to ensure CreateMLCEngine is called correctly.
            const engine = await webllm.CreateMLCEngine(
                modelId,
                { initProgressCallback: initProgressCallback }
            );

            engineRef.current = engine;

            setState(prev => ({
                ...prev,
                isLoading: false,
                isReady: true,
                text: "Ê∫ñÂÇôÂÆå‰∫ÜÔºÅ‚ú®",
                progress: 100
            }));
            releaseWakeLock();

        } catch (err: any) {
            console.error(`Attempt ${retryCount + 1} failed:`, err);

            if (retryCount < 3) {
                setState(prev => ({ ...prev, text: `ÂÜçË©¶Ë°å‰∏≠... (${retryCount + 1}/3)` }));
                setTimeout(() => initEngine(modelId, retryCount + 1), 3000);
            } else {
                setState(prev => ({
                    ...prev,
                    isLoading: false,
                    error: `„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ (${err.message || 'Unknown Error'}) ÈÄö‰ø°Áí∞Â¢É„ÅÆËâØ„ÅÑÂ†¥ÊâÄ„ÅßÂÜçË©¶Ë°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`,
                    text: "Error"
                }));
                releaseWakeLock();
            }
        }
    }, [state.isReady]);

    const generateReplyStream = useCallback(async (
        history: { role: "user" | "assistant" | "system", content: string }[],
        onUpdate: (currentText: string) => void,
        onComplete: (finalText: string) => void
    ) => {
        if (!engineRef.current || !state.isReady) {
            throw new Error("AI Engine not ready");
        }

        try {
            const messages = [
                { role: "system", content: SYSTEM_PROMPT },
                ...history
            ] as webllm.ChatCompletionMessageParam[];

            const chunks = await engineRef.current.chat.completions.create({
                messages,
                temperature: 0.8,
                stream: true,
                max_tokens: 150,
            });

            let fullText = "";
            for await (const chunk of chunks) {
                const delta = chunk.choices[0]?.delta.content || "";
                fullText += delta;
                onUpdate(fullText);
            }

            onComplete(fullText);

        } catch (err) {
            console.error("Generation failed:", err);
            const errorMsg = "„ÅÇ„Çâ„ÄÅÈõªÊ≥¢„ÅåÊÇ™„ÅÑ„Åø„Åü„ÅÑ... „ÇÇ„ÅÜ‰∏ÄÂ∫¶Ë®Ä„Å£„Å¶„Åè„Çå„ÇãÔºü";
            onUpdate(errorMsg);
            onComplete(errorMsg);
        }
    }, [state.isReady]);

    return {
        ...state,
        initEngine,
        generateReplyStream
    };
};
